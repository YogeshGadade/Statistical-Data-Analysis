{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Business Problem: \n",
        "Estimate the proportion of customers who will churn (stop shopping at the store) in the next month based on the behavior of a sample of customers \n",
        "\n",
        "Solution: \n",
        "1. Will try to estimate the range of customers \n",
        "2. will try to confirm my calculate confidance interval, perform hypothesis testing confirming sample of customers representing actual customer distribution using inferential statistics in Python."
      ],
      "metadata": {
        "id": "QyZvkE82jrKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Implementation Steps: \n",
        "Following code first loads data (created for this analysis) loads customer data, and then calculates the number of customers who have churned, the total number of customers, and the proportion of customers who have churned. \n",
        "\n",
        "Then, using the sample proportion and the sample size, it calculates the standard error, margin of error, and the lower and upper bounds of the 95% confidence interval for the proportion of customers who will churn in the next month. \n",
        "\n",
        "And finally, it uses a binomial test to get the p-value of the hypothesis that the proportion of customers who will churn in the next month is equal to the proportion of customers who have churned so far."
      ],
      "metadata": {
        "id": "z-uV8L54dofu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import binom_test"
      ],
      "metadata": {
        "id": "k-HDd91YvVA7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Synthetic Data:\n",
        "\n",
        "## Method 1: \n",
        "### import random \n",
        "### customers = {'customer_id': list(range(1, num_customers+1)),\n",
        "###             'churned': [random.choice([True, False]) for i in range(num_customers)]\n",
        "###            }\n",
        "### data = pd.DataFrame(customer_data)\n",
        "\n",
        "## Method 2:\n",
        "# set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# create an array of customer_id\n",
        "customer_id = np.random.randint(1,10000,size=1004)\n",
        "\n",
        "# create an array of customer_type\n",
        "customer_type = np.random.choice(['A','B','C','D'], size=1004)\n",
        "\n",
        "# create an array of purchased_amount\n",
        "purchased_amount = np.random.normal(1000,50, size=1004)\n",
        "\n",
        "# create an array of churned\n",
        "churned = np.random.choice([True,False], size=1004, p=[0.2, 0.8])\n",
        "\n",
        "# create a dataframe\n",
        "customer_data = pd.DataFrame({'customer_id': customer_id,\n",
        "                             'customer_type': customer_type,\n",
        "                             'purchased_amount': purchased_amount,\n",
        "                             'churned': churned})\n",
        "\n",
        "# Add duplicate rows\n",
        "customer_data = customer_data.append(customer_data.iloc[:2, :], ignore_index=True)\n",
        "\n",
        "# Add rows with missing values\n",
        "customer_data.iloc[1002, 1] = np.nan\n",
        "customer_data.iloc[1003, 2] = np.nan\n",
        "\n",
        "print(\"Original Data : \\n\", customer_data.head())\n",
        "# Data Preprocessing\n",
        "customer_data.drop_duplicates(subset='customer_id', keep=False, inplace=True)\n",
        "customer_data.dropna(inplace=True)\n",
        "print(\"Preprocessed Data : \\n\", customer_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYUf5UNKE773",
        "outputId": "7f8ed26e-0293-4e33-b077-e9313c49f852"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data : \n",
            "    customer_id customer_type  purchased_amount  churned\n",
            "0         7271             D       1090.599006    False\n",
            "1          861             D       1017.896286    False\n",
            "2         5391             D       1019.663158    False\n",
            "3         5192             A       1034.072254    False\n",
            "4         5735             D        971.026836    False\n",
            "Preprocessed Data : \n",
            "    customer_id customer_type  purchased_amount  churned\n",
            "2         5391             D       1019.663158    False\n",
            "3         5192             A       1034.072254    False\n",
            "4         5735             D        971.026836    False\n",
            "5         6266             A       1054.450499    False\n",
            "6          467             D       1005.726668    False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As data is in desired format \n",
        "\n",
        "# calculate the lower and upper bounds of the 95% confidence interval:\n",
        "## 1. calculate the number of customers who have churned\n",
        "num_churned = customer_data[customer_data['churned'] == True].shape[0]\n",
        "\n",
        "## 2. calculate the total number of customers\n",
        "num_total = customer_data.shape[0]\n",
        "\n",
        "## 3. calculate the proportion of customers who have churned\n",
        "prop_churned = num_churned / num_total\n",
        "\n",
        "## 4. calculate the standard error of the proportion\n",
        "### se = sqrt(hypothesized proportion * (1 - hypothesized proportion) / sample size)\n",
        "se = (prop_churned * (1 - prop_churned) / num_total)**0.5\n",
        "\n",
        "## 5. calculate the margin of error\n",
        "z_score=1.96 # since we want 95% CI\n",
        "me = z_score * se\n",
        "\n",
        "## 6. calculate the lower and upper bounds of the 95% confidence interval\n",
        "lower_bound = prop_churned - me\n",
        "upper_bound = prop_churned + me\n",
        "\n",
        "print(\"The proportion of customers who will churn in the next month is estimated to be between\", lower_bound, \"and\", upper_bound)\n",
        "\n",
        "# Now that we have the interval, we can use a binomial test\n",
        "p_value = binom_test(num_churned, num_total, prop_churned, alternative = 'two-sided')\n",
        "\n",
        "print(\"The p_value for the hypothesis test is\", p_value)\n"
      ],
      "metadata": {
        "id": "2FUG1QEBeOHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c43bbb-45e2-4c10-90d9-392347bcd4e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The proportion of customers who will churn in the next month is estimated to be between 0.16418391153900816 and 0.21539433485388862\n",
            "The p_value for the hypothesis test is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis Testing: \n",
        "\n",
        "However, since the sample proportion is just an estimate and not the actual population proportion, there is a chance that the sample proportion could be different from the true population proportion. The p-value helps us to quantify this chance.\n",
        "\n",
        "A p-value is the probability of observing a sample proportion as extreme or more extreme than the one observed, assuming that the null hypothesis is true. In this case, the null hypothesis is that the proportion of customers who will churn in the next month is equal to the proportion of customers who have churned so far. If the p-value is less than a certain threshold (usually 0.05), it means that the sample proportion is unlikely to have occurred by chance alone and therefore we can reject the null hypothesis and conclude that the sample proportion is different from the true population proportion.\n",
        "\n",
        "In other words, p-value is a measure of how much evidence we have against the null hypothesis. If p-value is low, that means we have enough evidence to reject the null hypothesis, which means that we have enough evidence to say that the sample proportion is different from the true population proportion.\n",
        "\n",
        "In this case, the p-value is 1.0, which means that the sample proportion is consistent with the population proportion, and thus the customer churn rate is consistent with what was expected."
      ],
      "metadata": {
        "id": "PZRxofsVnjfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result Analysis: \n",
        "The output shows that the proportion of customers who will churn in the next month is estimated to be between 16.41% and 21.54%. The 95% confidence interval gives us a range of plausible values for the true population proportion of customers who will churn in the next month, based on the sample data. The lower bound of the interval is 0.16418391153900816 and the upper bound is 0.21539433485388862\n",
        "The p-value of 1.0 means that the probability of observing the current sample proportion, or a more extreme proportion, assuming that the null hypothesis is true, is 1.0. This means that the sample proportion is consistent with the population proportion, and therefore the null hypothesis is accepted.\n",
        "\n",
        "In other words, it means that the sample proportion of customer churn is not significantly different from the population proportion, which is 0.1, and thus the customer churn rate is consistent with what was expected."
      ],
      "metadata": {
        "id": "n3OXmeP6jS7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improvements In the above implementation:\n",
        "\n",
        "code I provided earlier, includes few preprocessing steps. However, if you are working with real-world data, some preprocessing steps might be necessary before running the analysis. These steps might include:"
      ],
      "metadata": {
        "id": "Sr1-TfAcjZ3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load data\n",
        "#data = pd.read_csv(\"customer_data.csv\")\n",
        "# set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# create an array of customer_id\n",
        "customer_id = np.random.randint(1,10000,size=1004)\n",
        "\n",
        "# create an array of customer_type\n",
        "customer_type = np.random.choice(['A','B','C','D'], size=1004)\n",
        "\n",
        "# create an array of purchased_amount\n",
        "purchase_amount = np.random.normal(1000,50, size=1004)\n",
        "\n",
        "# create an array of churned\n",
        "churned = np.random.choice([True,False], size=1004, p=[0.2, 0.8])\n",
        "\n",
        "# create a dataframe\n",
        "customer_data = pd.DataFrame({'customer_id': customer_id, 'customer_type': customer_type,\n",
        "                             'purchase_amount': purchase_amount, 'churned': churned})\n",
        "\n",
        "#customer_data = {\n",
        "#    'customer_id': np.random.randint(1, 100000, size=1004),\n",
        "#    'purchased_amount': np.random.rand(1004) * 100,\n",
        "#    'churned': np.random.choice([True, False], size=1004, p=[0.2, 0.8])\n",
        "#}\n",
        "\n",
        "# adding duplicate rows\n",
        "#customer_data = pd.DataFrame(customer_data)\n",
        "#customer_data = customer_data.append(customer_data.iloc[:2,:])\n",
        "\n",
        "# adding null values\n",
        "#customer_data.iloc[2,1] = np.nan\n",
        "#customer_data.iloc[3,2] = np.nan\n",
        "\n",
        "\n",
        "# Add duplicate rows\n",
        "customer_data = customer_data.append(customer_data.iloc[:2, :], ignore_index=True)\n",
        "\n",
        "# Add rows with missing values\n",
        "customer_data.iloc[1002, 1] = np.nan\n",
        "customer_data.iloc[1003, 2] = np.nan\n",
        "\n",
        "print(\"Original Data : \\n\", customer_data.head())\n",
        "\n",
        "\n",
        "\n",
        "# Data cleaning\n",
        "customer_data.dropna(inplace=True) # remove rows with missing values\n",
        "customer_data.drop_duplicates(inplace=True) # remove duplicate rows\n",
        "\n",
        "# Data transformation\n",
        "customer_data['churned'] = customer_data['churned'].astype(int) # convert churned column to numeric\n",
        "customer_data = pd.get_dummies(customer_data, columns=['customer_type']) # convert customer_type column to dummy variables\n",
        "\n",
        "# Feature engineering\n",
        "customer_data['average_purchase_amount'] = customer_data.groupby('customer_id')['purchase_amount'].transform('mean') # create a new variable for average purchase amount\n",
        "\n",
        "# Data splitting\n",
        "X = customer_data.drop(columns=['churned'])\n",
        "y = customer_data['churned']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Data normalization\n",
        "scaler = StandardScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "3Ay7ZN_Rxm6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "124ea9c3-cbef-48ce-97f4-974613d277d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data : \n",
            "    customer_id customer_type  purchase_amount  churned\n",
            "0         7271             D      1090.599006    False\n",
            "1          861             D      1017.896286    False\n",
            "2         5391             D      1019.663158    False\n",
            "3         5192             A      1034.072254    False\n",
            "4         5735             D       971.026836    False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code first loads customer data, and then performs several preprocessing steps:\n",
        "\n",
        "1. Data cleaning: Removes any missing values and duplicate rows\n",
        "2. Data transformation: converts the churned column to numeric and customer_type column to dummy variables\n",
        "3. Feature engineering: creates a new variable for average purchase amount by customer_id\n",
        "4. Data splitting: Splits the data into a training set (80%) and a test set (20%)\n",
        "5. Data normalization: normalizes the data using StandardScaler.\n",
        "\n",
        "Note that this code is just an example, and the specific preprocessing steps required will depend on the nature and quality of the data, and on the goals of the analysis. Also, it's always a good practice to check the data and do some data exploration, cleaning, and transformation when required."
      ],
      "metadata": {
        "id": "Bk2WkvD2ja28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Possible challanges: \n",
        "There are several challenges that can arise when trying to find customer churning, including:\n",
        "\n",
        "1. Defining churn: Churn can be defined in different ways, depending on the context and the goals of the analysis. For example, churn might be defined as customers who cancel their subscription, stop using the service, or switch to a competitor. Defining churn in a clear and consistent way is important for the analysis.\n",
        "2. Data availability and quality: In order to identify customers who are at risk of churning, it is important to have access to data on their behavior and demographics. However, in many cases, data might be missing, incomplete, or of poor quality, which can make it difficult to conduct the analysis.\n",
        "3. Identifying the causes of churn: Churn can be caused by a variety of factors, such as poor service quality, high prices, or lack of features. Identifying the specific causes of churn can be challenging, as customers may have different reasons for churning, and a single customer may have multiple reasons.\n",
        "4. Handling class imbalance: Churn is typically a rare event, which means that the number of customers who churn is much smaller than the number of customers who do not churn. This can lead to class imbalance in the data, which can make it difficult to train accurate predictive models.\n",
        "5. Handling temporal data: Churn prediction often relies on time-series data, where the information on the past behavior of customers is used to predict the future behavior of customers. Such data can be challenging to handle, as it can be affected by seasonality, trends, and irregular patterns.\n",
        "6. Model evaluation and selection: Churn prediction models are often evaluated based on their accuracy or AUC-ROC scores, but these metrics may not be the most appropriate for imbalanced data and the business goals. There are other metrics like precision, recall, F1-score and Lift that are more suited to evaluate the model in such scenarios.\n",
        "7. Model interpretability: Model interpretability is crucial for understanding what factors are driving customer churn and how to take action to retain customers. Some models, such as decision trees or rule-based models, are more interpretable than others, such as neural networks or random forests."
      ],
      "metadata": {
        "id": "By8kH2hCjfo1"
      }
    }
  ]
}